{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codegen_sources.model.translate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_code = \"using System;using System.Linq;namespace AOJ{class Program{public static void Main(string[] args){foreach(var i in Enumerable.Range(1, 9)){foreach(var j in Enumerable.Range(1, 9)){Console.WriteLine('{0}x{1}={2}', i, j, i*j);}}}}}\"\n",
    "cpp_code = \"#include<iostream>using namespace std;int main(){cin.tie(0);ios::sync_with_stdio(false);for(int i=1;i<10;i++)for(int j=1;j<10;j++)cout << i << 'x' << j << '='' << i*j << '\\n';}\"\n",
    "java_code = \"class Main {public static void main(String[] args){for(int i = 1; i < 10; ++i){for(int j = 1; j < 10; ++j){System.out.println(Integer.toString(i)+'x'+Integer.toString(j)+'='+Integer.toString(i*j));}}}} \"\n",
    "python_code = \"\"\"for i in range(1, n):\\n\\tcounter = 1\\n\\twhile ((i - counter) >= 0 and\\n\\t\\tA[i] >= A[i - counter]):\\n\\t\\tcounter += ans[i - counter]\\n\\tans[i] = counter\"\"\"\n",
    "js_code = \"prefix[0] = prefix[1] = 0;for (let p = 2; p <= MAX; p++) {prefix[p] = prefix[p - 1];if (prime[p])prefix[p]++;}}\"\n",
    "php_code = \"for ($p = 2; $p <= $MAX; $p++){$prefix[$p] = $prefix[$p - 1];if ($prime[$p])$prefix[$p]++;}}\"\n",
    "c_code = \"\"\"void print(struct Node *root){if (root != NULL){print(root->left);printf(\"%d \",root->data);print(root->right);}}\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_utils import *\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codegen_sources.model.classification_model import Classifier\n",
    "import torch\n",
    "\n",
    "artifacts_path=\"/home/mingzhu/CodeModel/CodeGen/code_corrption/last_token_segmented/java\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cls_model = Classifier(artifacts_path, device)\n",
    "\n",
    "seqs = [\"while ( n < 10 )\",\n",
    "\"import java . import\",\n",
    "\"import java . util .* ;\",\n",
    "\"import java . util .* ; class\",\n",
    "\"static void printTwoOdd static\",\n",
    "\"static void void ( int arr, x (\"]\n",
    "\n",
    "\n",
    "preds, probs, logits = cls_model.classify(seqs)\n",
    "\n",
    "print(\"Preds: \", preds)\n",
    "print(\"Probs: \", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    def __init__(self, model_path, BPE_path):\n",
    "        # reload model\n",
    "        reloaded = torch.load(model_path, map_location=\"cpu\")\n",
    "        # change params of the reloaded model so that it will\n",
    "        # relaod its own weights and not the MLM or DOBF pretrained model\n",
    "        reloaded[\"params\"][\"reload_model\"] = \",\".join([model_path] * 2)\n",
    "        reloaded[\"params\"][\"lgs_mapping\"] = \"\"\n",
    "        reloaded[\"params\"][\"reload_encoder_for_decoder\"] = False\n",
    "        self.reloaded_params = AttrDict(reloaded[\"params\"])\n",
    "\n",
    "        # build dictionary / update parameters\n",
    "        self.dico = Dictionary(\n",
    "            reloaded[\"dico_id2word\"], reloaded[\"dico_word2id\"], reloaded[\"dico_counts\"]\n",
    "        )\n",
    "        assert self.reloaded_params.n_words == len(self.dico)\n",
    "        assert self.reloaded_params.bos_index == self.dico.index(BOS_WORD)\n",
    "        assert self.reloaded_params.eos_index == self.dico.index(EOS_WORD)\n",
    "        assert self.reloaded_params.pad_index == self.dico.index(PAD_WORD)\n",
    "        assert self.reloaded_params.unk_index == self.dico.index(UNK_WORD)\n",
    "        assert self.reloaded_params.mask_index == self.dico.index(MASK_WORD)\n",
    "\n",
    "        # build model / reload weights (in the build_model method)\n",
    "        encoder, decoder = build_model(self.reloaded_params, self.dico)\n",
    "        self.encoder = encoder[0]\n",
    "        self.decoder = decoder[0]\n",
    "        self.encoder.cuda()\n",
    "        self.decoder.cuda()\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        # reload bpe\n",
    "        if getattr(self.reloaded_params, \"roberta_mode\", False):\n",
    "            print('roberta')\n",
    "            self.bpe_model = RobertaBPEMode()\n",
    "        else:\n",
    "            print('non roberta')\n",
    "            self.bpe_model = FastBPEMode(\n",
    "                codes=os.path.abspath(BPE_path), vocab_path=None\n",
    "            )\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        all_data,\n",
    "        lang1,\n",
    "        lang2,\n",
    "        precondition_topk, \n",
    "        condition_lambda,\n",
    "        suffix1=\"_sa\",\n",
    "        suffix2=\"_sa\",\n",
    "        n=1,\n",
    "        beam_size=1,\n",
    "        sample_temperature=None,\n",
    "        device=\"cuda:0\",\n",
    "        cont=False,\n",
    "        \n",
    "    ):\n",
    "\n",
    "        # Build language processors\n",
    "        assert lang1 in {\"cpp\", \"java\", \"python\", \"csharp\", 'javascript', 'php', 'c'}, lang1\n",
    "        assert lang2 in {\"cpp\", \"java\", \"python\", \"csharp\", 'javascript', 'php', 'c'}, lang2\n",
    "        so_path = \"/home/mingzhu/CodeModel/CodeGen/codegen_sources/preprocessing/lang_processors\"\n",
    "        src_lang_processor = LangProcessor.processors[lang1](\n",
    "            root_folder=so_path\n",
    "        )\n",
    "        tokenizer = src_lang_processor.tokenize_code\n",
    "        tgt_lang_processor = LangProcessor.processors[lang2](\n",
    "            root_folder=so_path\n",
    "        )\n",
    "        detokenizer = tgt_lang_processor.detokenize_code\n",
    "\n",
    "        lang1 += suffix1\n",
    "        lang2 += suffix2\n",
    "\n",
    "#         assert (\n",
    "#             lang1 in self.reloaded_params.lang2id.keys()\n",
    "#         ), f\"{lang1} should be in {self.reloaded_params.lang2id.keys()}\"\n",
    "#         assert (\n",
    "#             lang2 in self.reloaded_params.lang2id.keys()\n",
    "#         ), f\"{lang2} should be in {self.reloaded_params.lang2id.keys()}\"\n",
    "        \n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            lang1_id = self.reloaded_params.lang2id[lang1]\n",
    "            lang2_id = self.reloaded_params.lang2id[lang2]\n",
    "\n",
    "            results_l = []\n",
    "            for i, input in tqdm(enumerate(all_data)):\n",
    "                # Convert source code to ids\n",
    "                tokens = [t for t in tokenizer(input)]\n",
    "#                 print(f\"Tokenized {params.src_lang} function:\")\n",
    "#                 print(\"before bpe\", tokens)\n",
    "                tokens = self.bpe_model.apply_bpe(\" \".join(tokens)).split()\n",
    "#                 print(\"after bpe\", tokens)\n",
    "                tokens = [\"</s>\"] + tokens + [\"</s>\"]\n",
    "                input = \" \".join(tokens)\n",
    "#                 inputs.append(input_toks)\n",
    "            \n",
    "                # Create torch batch\n",
    "                len1 = len(input.split())\n",
    "                len1 = torch.LongTensor(1).fill_(len1).to(device)\n",
    "                inds = [self.dico.index(w) for w in input.split()]\n",
    "#                 print('inds', inds)\n",
    "                x1 = torch.LongTensor(inds).to(\n",
    "                    device\n",
    "                )[:, None]\n",
    "                langs1 = x1.clone().fill_(lang1_id)\n",
    "\n",
    "                # Encode\n",
    "                enc1 = self.encoder(\"fwd\", x=x1, lengths=len1, langs=langs1, causal=False)\n",
    "                enc1 = enc1.transpose(0, 1)\n",
    "                if n > 1:\n",
    "                    enc1 = enc1.repeat(n, 1, 1)\n",
    "                    len1 = len1.expand(n)\n",
    "\n",
    "                # Decode\n",
    "                if beam_size == 1:\n",
    "                    if cont:\n",
    "                        x2, len2 = self.decoder.generate_cont(\n",
    "                            cont,\n",
    "                            cls_model,\n",
    "                            detok, \n",
    "                            self.dico,\n",
    "                            detokenizer,\n",
    "                            precondition_topk, \n",
    "                            condition_lambda,\n",
    "                            enc1,\n",
    "                            len1,\n",
    "                            lang2_id,\n",
    "                            max_len=int(\n",
    "                                min(self.reloaded_params.max_len, 3 * len1.max().item() + 10)\n",
    "                            ),\n",
    "                            sample_temperature=sample_temperature,\n",
    "                        )\n",
    "                    else:\n",
    "                        x2, len2 = self.decoder.generate(\n",
    "                            enc1,\n",
    "                            len1,\n",
    "                            lang2_id,\n",
    "                            max_len=int(\n",
    "                                min(self.reloaded_params.max_len, 3 * len1.max().item() + 10)\n",
    "                            ),\n",
    "                            sample_temperature=sample_temperature,\n",
    "                        )\n",
    "                else:\n",
    "                    x2, len2, _ = self.decoder.generate_beam(\n",
    "                        enc1,\n",
    "                        len1,\n",
    "                        lang2_id,\n",
    "                        max_len=int(\n",
    "                            min(self.reloaded_params.max_len, 3 * len1.max().item() + 10)\n",
    "                        ),\n",
    "                        early_stopping=False,\n",
    "                        length_penalty=1.0,\n",
    "                        beam_size=beam_size,\n",
    "                    )\n",
    "\n",
    "                # Convert out ids to text\n",
    "                tok = []\n",
    "                for i in range(x2.shape[1]):\n",
    "                    wid = [self.dico[x2[j, i].item()] for j in range(len(x2))][1:]\n",
    "                    wid = wid[: wid.index(EOS_WORD)] if EOS_WORD in wid else wid\n",
    "                    if getattr(self.reloaded_params, \"roberta_mode\", False):\n",
    "                        tok.append(restore_roberta_segmentation_sentence(\" \".join(wid)))\n",
    "                    else:\n",
    "                        tok.append(\" \".join(wid).replace(\"@@ \", \"\"))\n",
    "                results = []\n",
    "                for t in tok:\n",
    "                    results.append(fix_format(detokenizer(t)))\n",
    "                results_l.append(results)\n",
    "            return results_l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def fix_format(y):\n",
    "    y = y.replace(\" @ @\", \"@@\").replace(\"@ @\", \"@@\")\n",
    "    x = re.sub('[\\n]{2,}','\\n',y)\n",
    "    xs = x.split('\\n')\n",
    "#     remove empty lines\n",
    "    s = \"\\n\".join([t for t in xs if len(t.strip()) > 0])\n",
    "#     remove linebreak between ()\n",
    "    s = re.sub(r'\\n(?=[^()]*\\))', '', s)\n",
    "#     remove linebreak between []\n",
    "    s = re.sub(r'\\n(?=[^\\[\\]]*\\])', '', s)\n",
    "    s = re.sub(r'[\\t| ]+(?=[^()]*\\))', ' ', s)\n",
    "    s = re.sub(r'[\\t| ]+(?=[^\\[\\]]*\\])', ' ', s)\n",
    "#     reduce unnecessary black space\n",
    "    s = re.sub('[ ]{2,}',' ',s)\n",
    "    return s\n",
    "\n",
    "def detok(x2, dico, detokenizer):\n",
    "#     x2: bs * candidates * seq_len\n",
    "#     print(x2)\n",
    "    with torch.no_grad():\n",
    "        tok = []\n",
    "        bz = x2.shape[0]\n",
    "        cand_size = x2.shape[1]\n",
    "        seq_len = x2.shape[2]\n",
    "        for i in range(bz):\n",
    "            tok_cand = []\n",
    "            for j in range(cand_size):\n",
    "                wid = [dico[x2[i, j, k].item()] for k in range(seq_len)][1:]\n",
    "#                 print(\"wid\", wid)\n",
    "                wid = wid[: wid.index(EOS_WORD)] if EOS_WORD in wid else wid\n",
    "                dec_seq = \" \".join(wid).replace(\"@@ \", \"\")\n",
    "#                 print(\"dec_seq\", dec_seq)\n",
    "                detoc_seq = detokenizer(dec_seq)\n",
    "#                 print(\"detoc_seq\",detoc_seq)\n",
    "                fixed_seq = fix_format(detoc_seq)\n",
    "#                 print(\"fixed_seq\",fixed_seq)\n",
    "                tok_cand.append(fixed_seq)\n",
    "                \n",
    "            tok.append(tok_cand)\n",
    "        return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "animated-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_list(a, precondition_topk, batch_size):\n",
    "    batches = np.array_split(a, (precondition_topk//batch_size) + 1, axis=-1)\n",
    "    max_tok_size = 5\n",
    "    for i, batch in enumerate(batches):\n",
    "        batch = batch.reshape(-1)\n",
    "        batches[i] = batch\n",
    "        if len(batch[0].split()) > max_tok_size:\n",
    "            for j, seq in enumerate(batch):\n",
    "                lines = seq.split(\"\\n\")\n",
    "                batches[i][j] = \" \".join(lines[-4:])\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "freelance-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_list(a, precondition_topk, bsize, cls_bsize, max_tok_size=20):\n",
    "    num_batch = precondition_topk//cls_bsize\n",
    "    if precondition_topk%cls_bsize != 0:\n",
    "        num_batch += 1\n",
    "#     num_batch * (bsize * cls_bsize)\n",
    "    batches = np.array_split(a, num_batch, axis=-1)\n",
    "    new_batches = []\n",
    "    for i, batch in enumerate(batches):\n",
    "#         print(\"batch.shape\", batch.shape)\n",
    "        batch_flat = batch.reshape(-1)\n",
    "        batches[i] = batch_flat\n",
    "        if len(batch_flat[0].split()) > max_tok_size:\n",
    "            for j, seq in enumerate(batch_flat):\n",
    "                lines = seq.split(\"\\n\")\n",
    "                new_seq = \" \".join(lines[-4:])\n",
    "                batches[i][j] = new_seq\n",
    "            new_batches.append(batches[i].reshape((bsize, cls_bsize)))\n",
    "    a_trunc = np.concatenate(new_batches, axis=1)\n",
    "    return batches, a_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "spread-manhattan",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'amax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-696e7428cd54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'amax'"
     ]
    }
   ],
   "source": [
    "probs1.amax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "recreational-scale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 10, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs1 = np.random.rand(13, 10, 2)\n",
    "probs2 = np.random.rand(13, 10, 2)\n",
    "np.concatenate((probs1, probs2), axis=1).shape\n",
    "np.concatenate([probs1], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "complex-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seqs = np.random.rand(13, 150)\n",
    "precondition_topk = 150\n",
    "cutoff = 100\n",
    "seqs_batches = []\n",
    "if precondition_topk > cutoff:\n",
    "    for i in range(precondition_topk//cutoff):\n",
    "        seqs_batches.append(seqs[:, i*cutoff:(i+1) * cutoff])\n",
    "    remaining = precondition_topk%cutoff\n",
    "    if remaining > 0:\n",
    "        seqs_batches.append(seqs[:, -remaining:])\n",
    "    \n",
    "# cls_preds, cls_probs, cls_logits = [], [], []\n",
    "# for seqs_batch in seqs_batches:\n",
    "#     seqs_reshape = [seq for batch in seqs_batch for seq in batch]\n",
    "# #                     assert len(seqs_reshape) == precondition_topk * bsize\n",
    "#     max_tok_size = 20\n",
    "#     if len(seqs_reshape[0].split()) > max_tok_size:\n",
    "#         for i, seq in enumerate(seqs_reshape):\n",
    "#             lines = seq.split(\"\\n\")\n",
    "#             seqs_reshape[i] = \" \".join(lines[-4:])\n",
    "\n",
    "#     cls_preds_batch, cls_probs_batch, cls_logits_batch = cls_model.classify(seqs_reshape)\n",
    "#     cls_preds += cls_preds_batch\n",
    "#     cls_probs += cls_probs_batch\n",
    "# pos_probs = cls_probs.reshape((bsize, -1, 2))[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "junior-roberts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqs_batches[0][0])\n",
    "len(seqs_batches[1][1])\n",
    "# remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "lang1 = \"cpp\" #\n",
    "lang2 = \"java\"\n",
    "sys.argv = ['codegen_sources.model.translate', \n",
    "            '--src_lang', lang1, '--tgt_lang', lang2, \n",
    "            '--model_path', '<model_path>', '--beam_size', '1']\n",
    "parser = get_parser()\n",
    "params = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'dumppath1/transcoder_dobf_g4g_Python_Java/nzl4eeae7s/best-valid_python_sa-java_sa_mt_bleu.pth'\n",
    "# dumppath1/transcoder_g4g_1_Python_Java/s9pbal578r/best-valid_python_sa-java_sa_mt_bleu.pth\n",
    "model_path = \"dumppath1/transcoder_dobf_g4g_beam_10_C++_Java/q3sr66wo0u/best-valid_cpp_sa-java_sa_mt_bleu.pth\"\n",
    "# dumppath1/transcoder_g4g_1_C++_Java/8npxtk0klm/best-valid_cpp_sa-java_sa_mt_bleu.pth \n",
    "model_path = \"dumppath1/transcoder_g4g_program_transfer_C++_Java/m9f4n3mlpl/best-valid_cpp_sa-java_sa_mt_bleu.pth\"\n",
    "# model_path = \"TransCoder_model_2.pth\"\n",
    "translator = Translator(\n",
    "    model_path, \n",
    "    Fast_BPE_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_code = \"\"\"#include <bits/stdc++.h> NEW_LINE using namespace std ; void printTwoOdd ( int arr [ ] , int size ) { int xor2 = arr [ 0 ] ; int set_bit_no ; int i ; int n = size - 2 ; int x = 0 , y = 0 ; for ( i = 1 ; i < size ; i ++ ) xor2 = xor2 ^ arr [ i ] ; set_bit_no = xor2 & ~ ( xor2 - 1 ) ; for ( i = 0 ; i < size ; i ++ ) { if ( arr [ i ] & set_bit_no ) x = x ^ arr [ i ] ; else y = y ^ arr [ i ] ; } cout << \" The ▁ two ▁ ODD ▁ elements ▁ are ▁ \" << x << \" ▁ & ▁ \" << y ; } int main ( ) { int arr [ ] = { 4 , 2 , 4 , 5 , 2 , 3 , 3 , 1 } ; int arr_size = sizeof ( arr ) / sizeof ( arr [ 0 ] ) ; printTwoOdd ( arr , arr_size ) ; return 0 ; }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../g4g/pair_data_tok_full/Java-C++/test-Java-C++-tok.cpp\"\n",
    "with open(data_path) as infile:\n",
    "    lines = infile.readlines()\n",
    "    cpp_code = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-timing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = cpp_code[:10]\n",
    "beam_size = 1\n",
    "with torch.no_grad():\n",
    "    outputs_cont = translator.translate(\n",
    "            all_data,\n",
    "            lang1=lang1,\n",
    "            lang2=lang2,\n",
    "            beam_size=beam_size,\n",
    "            precondition_topk=10, \n",
    "            condition_lambda=0.5,\n",
    "            cont=True,\n",
    "            \n",
    "        )\n",
    "for batch in outputs_cont:\n",
    "    for seq in batch:\n",
    "        print(seq)\n",
    "        print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = cpp_code\n",
    "beam_size = 5\n",
    "with torch.no_grad():\n",
    "    outputs = translator.translate(\n",
    "            all_data,\n",
    "            lang1=lang1,\n",
    "            lang2=lang2,\n",
    "            beam_size=beam_size,\n",
    "            precondition_topk=10, \n",
    "            condition_lambda=0.5,\n",
    "            cont=False,\n",
    "        )\n",
    "for batch in outputs:\n",
    "    for seq in batch:\n",
    "        print(seq)\n",
    "        print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_path = \"../g4g/pair_data_tok_full/Java-C++/test-Java-C++-tok.java\"\n",
    "with open(ref_path) as infile:\n",
    "    lines = infile.readlines()\n",
    "    java_code = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_corpus = []\n",
    "for line in java_code:\n",
    "    references_corpus.append([line.split(\" \")])\n",
    "#     print(references_corpus)\n",
    "\n",
    "outputs_eval = [seq[0].split(\" \") for seq in outputs]\n",
    "# outputs_cont_eval = [seq[0].split(\" \") for seq in outputs_cont]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(references_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "b1 = bleu_score(outputs_eval[:-1], references_corpus)\n",
    "# b2 =  bleu_score(outputs_cont_eval, references_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = 'python'\n",
    "lang2 = 'java'\n",
    "so_path = \"/home/mingzhu/CodeModel/CodeGen/codegen_sources/preprocessing/lang_processors\"\n",
    "src_lang_processor = LangProcessor.processors[lang1](\n",
    "    root_folder=so_path\n",
    ")\n",
    "tokenizer = src_lang_processor.tokenize_code\n",
    "tgt_lang_processor = LangProcessor.processors[lang2](\n",
    "    root_folder=so_path\n",
    ")\n",
    "detokenizer = tgt_lang_processor.detokenize_code\n",
    "suffix1=\"_sa\"\n",
    "suffix2=\"_sa\"\n",
    "lang1 += suffix1\n",
    "lang2 += suffix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [python_code]\n",
    "with torch.no_grad():\n",
    "\n",
    "    lang1_id = self.reloaded_params.lang2id[lang1]\n",
    "    lang2_id = self.reloaded_params.lang2id[lang2]\n",
    "\n",
    "    results_l = []\n",
    "    for i, input in tqdm(enumerate(all_data)):\n",
    "        # Convert source code to ids\n",
    "        tokens = [t for t in tokenizer(input)]\n",
    "        print(f\"Tokenized {params.src_lang} function:\")\n",
    "#                 print(\"before bpe\", tokens)\n",
    "        tokens = self.bpe_model.apply_bpe(\" \".join(tokens)).split()\n",
    "        print(\"after bpe\", tokens)\n",
    "        tokens = [\"</s>\"] + tokens + [\"</s>\"]\n",
    "        input = \" \".join(tokens)\n",
    "#                 inputs.append(input_toks)\n",
    "\n",
    "        # Create torch batch\n",
    "        len1 = len(input.split())\n",
    "        len1 = torch.LongTensor(1).fill_(len1).to(device)\n",
    "        inds = [self.dico.index(w) for w in input.split()]\n",
    "#                 print('inds', inds)\n",
    "        x1 = torch.LongTensor(inds).to(\n",
    "            device\n",
    "        )[:, None]\n",
    "        langs1 = x1.clone().fill_(lang1_id)\n",
    "\n",
    "        # Encode\n",
    "        enc1 = self.encoder(\"fwd\", x=x1, lengths=len1, langs=langs1, causal=False)\n",
    "        enc1 = enc1.transpose(0, 1)\n",
    "        if n > 1:\n",
    "            enc1 = enc1.repeat(n, 1, 1)\n",
    "            len1 = len1.expand(n)\n",
    "\n",
    "        # Decode\n",
    "        if beam_size == 1:\n",
    "            x2, len2 = self.decoder.generate(\n",
    "                enc1,\n",
    "                len1,\n",
    "                lang2_id,\n",
    "                max_len=int(\n",
    "                    min(self.reloaded_params.max_len, 3 * len1.max().item() + 10)\n",
    "                ),\n",
    "                sample_temperature=sample_temperature,\n",
    "            )\n",
    "        else:\n",
    "            x2, len2, _ = self.decoder.generate_beam(\n",
    "                enc1,\n",
    "                len1,\n",
    "                lang2_id,\n",
    "                max_len=int(\n",
    "                    min(self.reloaded_params.max_len, 3 * len1.max().item() + 10)\n",
    "                ),\n",
    "                early_stopping=False,\n",
    "                length_penalty=1.0,\n",
    "                beam_size=beam_size,\n",
    "            )\n",
    "\n",
    "        # Convert out ids to text\n",
    "        tok = []\n",
    "        for i in range(x2.shape[1]):\n",
    "            wid = [self.dico[x2[j, i].item()] for j in range(len(x2))][1:]\n",
    "            wid = wid[: wid.index(EOS_WORD)] if EOS_WORD in wid else wid\n",
    "            if getattr(self.reloaded_params, \"roberta_mode\", False):\n",
    "                tok.append(restore_roberta_segmentation_sentence(\" \".join(wid)))\n",
    "            else:\n",
    "                tok.append(\" \".join(wid).replace(\"@@ \", \"\"))\n",
    "        results = []\n",
    "        for t in tok:\n",
    "            results.append(detokenizer(t))\n",
    "        results_l.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-warning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
