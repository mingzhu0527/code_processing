{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "healthy-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instant-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./huggingface_models/')\n",
    "sys.path.append('./utils/')\n",
    "from sample_utils import *\n",
    "from inference_utils import *\n",
    "from codenet_process_utils import *\n",
    "from self_training_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conditional-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "white-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + 'codenet_merged_filtered_dict.json') as infile:\n",
    "    merged_filtered_dict = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respected-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ 37126\n",
      "Java 10511\n",
      "Python 26911\n",
      "C# 3101\n",
      "C 12424\n"
     ]
    }
   ],
   "source": [
    "for lang in new_langs:\n",
    "    print(lang, len(merged_filtered_dict[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_dict\n",
    "# preds_lang_dict\n",
    "# filtered_dict\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-tuning",
   "metadata": {},
   "source": [
    "### Show Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"Java\"\n",
    "key = 9981 #9981\n",
    "code_dic = merged_filtered_dict[lang][key]['code_dic']\n",
    "program = code_dic['program_formatted']\n",
    "paras = code_dic['parameter_lists']\n",
    "return_types = code_dic['return_types']\n",
    "function_names = code_dic['function_names']\n",
    "functions = code_dic['functions']\n",
    "function = \"\\n\".join(functions)\n",
    "pieces = code_dic['program_pieces']\n",
    "piece = \"\".join(pieces)\n",
    "target_call = code_dic['target_call']\n",
    "# print(program)\n",
    "print(function)\n",
    "print(piece)\n",
    "print(function_names)\n",
    "print(return_types)\n",
    "print(paras)\n",
    "print(target_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"Java\"\n",
    "root1, graph1, graph_pruned1, graph_sibs1, graph_pruned_sibs1 = pipeline(code1, ast_parsers[lang1], lang1)\n",
    "# root1, graph1, graph_pruned1, graph_sibs1, graph_pruned_sibs1 = refine_graphs(root1, graph_pruned_sibs1)\n",
    "show_graph(root1, graph1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-reading",
   "metadata": {},
   "source": [
    "### Preprocess data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-apache",
   "metadata": {},
   "source": [
    "#### No tokenization preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proprietary-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing steps:\n",
    "# remove comments, empty lines format_codestring_codenet(codestring, lang)\n",
    "# replace new_lines notok_prepro(codestring, lang, is_plbart)\n",
    "# after decoding, do notok_detok notok_detok(codestring, lang, is_plbart)\n",
    "# do detok_format(codestring, detokenizer) to get detokenized version for Java and Python\n",
    "\n",
    "is_plbart = True\n",
    "# merged_filtered_dict = get_prepro_filtered_dict(merged_filtered_dict, is_plbart)\n",
    "merged_filtered_dict = get_prepro_filtered_dict(None, is_plbart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-cedar",
   "metadata": {},
   "source": [
    "#### Split into batches for self-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "average-appreciation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ [0, 12375, 24750, 37126]\n",
      "Java [0, 3503, 7006, 10511]\n",
      "Python [0, 8970, 17940, 26911]\n",
      "C# [0, 1033, 2066, 3101]\n",
      "C [0, 4141, 8282, 12424]\n"
     ]
    }
   ],
   "source": [
    "# Java-Python\n",
    "# 3 batches \n",
    "num_batchs = 3\n",
    "batch_split_dict = {}\n",
    "for lang in new_langs:\n",
    "    length = len(merged_filtered_dict[lang])\n",
    "    batch_size = length//num_batchs\n",
    "    batch_list = [i*batch_size for i in range(num_batchs+1)]\n",
    "    batch_list[-1] = length\n",
    "    batch_split_dict[lang] = batch_list\n",
    "    print(lang, batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caring-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_programs_dict = {}\n",
    "for lang in new_langs:\n",
    "    batch_list = batch_split_dict[lang]\n",
    "    batch_functions = []\n",
    "    for bid in range(num_batchs):\n",
    "        batch_dict = merged_filtered_dict[lang][batch_list[bid]:batch_list[bid+1]]\n",
    "        functions = []\n",
    "        for dic in batch_dict:\n",
    "            functions.append(dic['function_notok'])\n",
    "        batch_functions.append(functions)\n",
    "    batch_programs_dict[lang] = batch_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "coordinate-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"Java\"\n",
    "lang2 = \"Python\"\n",
    "batch_id = 0\n",
    "src_codes = batch_programs_dict[lang1][batch_id]\n",
    "tgt_codes = []\n",
    "# infer with src and tgt\n",
    "eval_examples, eval_features, eval_dataloader, model, tokenizer, args, decoder_sid = inference_prepro(\n",
    " lang1, lang2, model_type, device, src_codes, tgt_codes, None, tag, exp_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-pixel",
   "metadata": {},
   "source": [
    "### Pre-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fitted-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"codenet\"\n",
    "is_eval = False\n",
    "sample_size = 5\n",
    "temperature = 0.5\n",
    "tag = \"all\"\n",
    "model_type = \"plbart\"\n",
    "exp_suffix = \"_translation_exec_function/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "discrete-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "descending-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get programs_dict\n",
    "programs_dict = get_codenet_programs_dict(merged_filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "current-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get import_str_dict\n",
    "import_str_dict = {}\n",
    "for lang in new_langs:\n",
    "    all_imports, import_str = get_common_imports(lang, merged_filtered_dict)\n",
    "    import_str_dict[lang] = import_str\n",
    "import_str_dict[\"Java\"] = java_imports_str\n",
    "import_str_dict[\"C#\"] = csharp_imports_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-attack",
   "metadata": {},
   "source": [
    "#### Get preds of unpaired programs. Use script get_codenet_preds.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lang_pairs and preds\n",
    "lang_pairs = []\n",
    "for lang1 in new_langs:\n",
    "    for lang2 in new_langs:\n",
    "        if lang2 != lang1:\n",
    "            lang_pairs.append((lang1, lang2))\n",
    "# small_programs_dict = {x:[\"haha\"] for x in new_langs}\n",
    "preds_lang_dict = get_preds_lang_dict_codenet(lang_pairs, model_type, device, programs_dict, \n",
    "                                      sample_size, temperature, data_name, tag, exp_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_filtering_compilation(pids, eval_examples, eval_features, function_id_lang_dic,\n",
    "                       reverse_map_dict, result_id_dict_gold, lang, model_type, model, tokenizer, args, device,\n",
    "                       decoder_sid=None, is_eval=True, num_samples=5, temperature=0.5):\n",
    "    selected_eval_examples, selected_eval_dataloader = get_eval_data_by_pid(eval_examples, eval_features, \n",
    "                                                               pids, reverse_map_dict, args.eval_batch_size)\n",
    "    torch.cuda.empty_cache()\n",
    "    preds, eval_result = generation_multiple(selected_eval_examples, \n",
    "                                             selected_eval_dataloader, \n",
    "                                             model, tokenizer, args, device, \n",
    "                                             decoder_sid, is_eval, num_samples, temperature)\n",
    "    programs, program_id_dict, program_dict = prep_exec_hypo(preds, pids,\n",
    "                                                             function_id_lang_dic, lang, model_type)\n",
    "    lang_results = p_map(file_executors[lang], programs)\n",
    "    result_id_dict, result_key_dict, error_type_dict = result_mapping(lang_results, program_id_dict, \n",
    "                                                                      pids, lang)\n",
    "    buggy_pids, failed_test_pids, passed_hypo_dict = hypo_filtering(result_id_dict, \n",
    "                                                                    result_id_dict_gold, \n",
    "                                                                    result_key_dict, \n",
    "                                                                    program_dict)\n",
    "    print(len(buggy_pids),len(failed_test_pids), len(passed_hypo_dict), len(pids))\n",
    "    return buggy_pids, failed_test_pids, passed_hypo_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-charm",
   "metadata": {},
   "source": [
    "### Merge preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_lang_dict_all = {}\n",
    "for lang in new_langs:\n",
    "    dic_path = cached_path + \"plbart_all_\" + lang + \"_codenet_preds_lang_dict.pkl\"\n",
    "    if not os.path.exists(dic_path):\n",
    "        continue\n",
    "    with open(dic_path, 'rb') as infile:\n",
    "        preds_lang_dict_batch = pickle.load(infile)\n",
    "        for k, v in preds_lang_dict_batch.items():\n",
    "            print(lang, k)\n",
    "            preds_lang_dict_all[k] = v\n",
    "print(preds_lang_dict_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "classified-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "plbart_sample_path = cached_path + 'plbart_codenet_preds_lang_dict.pkl'\n",
    "with open(plbart_sample_path, 'rb') as infile:\n",
    "     preds_lang_dict_plbart = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "essential-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "codet5_sample_path = cached_path + 'codet5_codenet_preds_lang_dict.pkl'\n",
    "with open(codet5_sample_path, 'rb') as infile:\n",
    "     preds_lang_dict_codet5 = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-faith",
   "metadata": {},
   "source": [
    "### Hypo filtering loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-begin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lang_pairs = [('Python', 'Java'), ('Python', 'C#'),\n",
    "            ('Java', 'C#'), ('Java', 'C++'), ('Java', 'C'), \n",
    "            ('C#', 'Java'), ('C#', 'Python'), ('C#', 'C++'), ('C#', 'C')]\n",
    "call_dict = {}\n",
    "for lang1, lang2 in lang_pairs:\n",
    "    print(lang1, lang2)\n",
    "    preds = preds_lang_dict_all[(lang1, lang2)]\n",
    "    new_preds = get_dedup_preds(preds)\n",
    "    functions, function_id_dict = prep_exec_hypo_codenet(new_preds, lang1, lang2, \n",
    "                                                                 merged_filtered_dict, model_type)\n",
    "    call_list = get_hypo_call_list(functions, lang2)\n",
    "    call_dict[(lang1, lang2)] = [new_preds, functions, function_id_dict, call_list]\n",
    "    with open(cached_path + \"codenet_lang_pair_call_dict.pkl\", 'wb') as outfile:\n",
    "        pickle.dump(call_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "raising-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"plbart_codenet_lang_pair_call_dict.pkl\", 'rb') as infile:\n",
    "    call_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-broadway",
   "metadata": {},
   "source": [
    "#### Get all the filtered hypos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "developed-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lang_dict = {}\n",
    "for lang1, lang2 in call_dict.keys():\n",
    "    new_preds, functions, function_id_dict, call_list = call_dict[(lang1, lang2)] \n",
    "    filtered_dict = get_compiled_hypos(call_list, function_id_dict, merged_filtered_dict)\n",
    "    filtered_lang_dict[(lang1, lang2)] = filtered_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-belgium",
   "metadata": {},
   "source": [
    "#### Separate pids that have filtered hypos and pids that don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_dict = {}\n",
    "non_empty_dict = {}\n",
    "for lang1, lang2 in filtered_lang_dict.keys():\n",
    "    non_empty_dict[(lang1, lang2)] = []\n",
    "    empty_dict[(lang1, lang2)] = []\n",
    "    filtered_dict = filtered_lang_dict[(lang1, lang2)]\n",
    "    for pid, inds in filtered_dict.items():\n",
    "        if len(inds) > 0:\n",
    "            non_empty_dict[(lang1, lang2)].append(pid)\n",
    "        else:\n",
    "            empty_dict[(lang1, lang2)].append(pid)\n",
    "    print(lang1, lang2, len(non_empty_dict[(lang1, lang2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-passage",
   "metadata": {},
   "source": [
    "### Check filtered hypo quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "brown-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare input output and check quality\n",
    "# Quality is very good!\n",
    "# numerics translation is not accurate.\n",
    "lang1 = 'C++'\n",
    "lang2 = 'Python'\n",
    "new_preds, functions, function_id_dict, call_list = call_dict[(lang1, lang2)] \n",
    "filtered_dict = filtered_lang_dict[(lang1, lang2)]\n",
    "src_codes = programs_dict[lang1]\n",
    "src_codes_formatted = [x['function'] for x in merged_filtered_dict[lang1]]\n",
    "non_empty_list = non_empty_dict[(lang1, lang2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 112\n",
    "for lang1 in new_langs:\n",
    "    for lang2 in new_langs:\n",
    "        if lang2 == lang1:\n",
    "            continue\n",
    "        new_preds, functions, function_id_dict, call_list = call_dict[(lang1, lang2)] \n",
    "        filtered_dict = filtered_lang_dict[(lang1, lang2)]\n",
    "        src_codes = programs_dict[lang1]\n",
    "        src_codes_formatted = [x['function'] for x in merged_filtered_dict[lang1]]\n",
    "        non_empty_list = non_empty_dict[(lang1, lang2)]\n",
    "        key = non_empty_list[-1]\n",
    "        if len(filtered_dict[key]) > 0:\n",
    "            print(lang1, lang2)\n",
    "            print(detok_format(functions[filtered_dict[key][0]], file_detokenizers[lang2]))\n",
    "            print(src_codes_formatted[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-adolescent",
   "metadata": {},
   "source": [
    "### Process filtered hypo for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ideal-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = 'Java'\n",
    "lang2 = 'Python'\n",
    "new_preds, functions, function_id_dict, call_list = call_dict[(lang1, lang2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cubic-ending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public void run(J_TOKEN.io.InputStream in,J_TOKEN.io.PrintStream out){ J_TOKEN.util.Scanner sc = new J_TOKEN.util.Scanner(in); int a, b, i, j, dig; for(;sc.hasNextInt();){ a = sc.nextInt(); b = sc.nextInt(); dig = caldig(a + b); System.out.println(dig); } sc.close(); } private static int caldig(int n){ int i; for(i = 1;i < 10;i++){ if(n / 10 == 0)return i; n = n / 10; } return 0; }\n"
     ]
    }
   ],
   "source": [
    "for func in programs_dict[\"Java\"]:\n",
    "    if \"J_T\" in func:\n",
    "        print(func)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions, notok_prepro(codestring, lang, is_plbart)\n",
    "# remove empty lines (caused by tokenization)\n",
    "# save into paired files; create map_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "popular-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codenet_process_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_pair_dict = get_lang_pair_dict(call_dict, merged_filtered_dict, programs_dict, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "protecting-laugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang1 public static String calc(int[] tempInt){ J_TOKEN.util.Arrays.sort(tempInt); if((tempInt[0]*tempInt[0])+(tempInt[1]*tempInt[1])==(tempInt[2]*tempInt[2])){ return \"YES\"; }else{ return \"NO\"; } }\n"
     ]
    }
   ],
   "source": [
    "for lang_pair in lang_pair_dict.keys():\n",
    "    lang1, lang2 = lang_pair\n",
    "    new_src_codes, new_target_codes, new_pids = lang_pair_dict[lang_pair]\n",
    "    if lang1 == \"Java\":\n",
    "        for func in new_src_codes:\n",
    "            if \"J_T\" in func:\n",
    "                print(\"lang1\", func)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-explorer",
   "metadata": {},
   "source": [
    "#### Check alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "loved-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"codenet_src_hypo_pair_dict_plbart.pkl\", 'rb') as infile:\n",
    "    lang_pair_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "twenty-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_pair1 = (\"Java\", \"Python\")\n",
    "lang_pair2 = (\"Python\", \"Java\")\n",
    "src_codes1, target_codes1, pids1 = lang_pair_dict[lang_pair1]\n",
    "src_codes2, target_codes2, pids2 = lang_pair_dict[lang_pair2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "clear-eugene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Python', 'C++')\n",
      "def check ( n , array ) :\n",
      "    if 1.1 <= n :\n",
      "        array [ 0 ] += 1\n",
      "    elif 0.6 <= n < 1.1 :\n",
      "        array [ 1 ] += 1\n",
      "    elif 0.2 <= n < 0.6 :\n",
      "        array [ 2 ] += 1\n",
      "    else :\n",
      "        array [ 3 ] += 1\n",
      "void check ( int n , int array [ ] )\n",
      "{\n",
      "  if ( 1.0 <= n )\n",
      "  array [ 0 ] ++ ;\n",
      "  else if ( 1.0 <= n )\n",
      "  array [ 1 ] ++ ;\n",
      "  else if ( 0.2 <= n )\n",
      "  array [ 2 ] ++ ;\n",
      "  else\n",
      "  array [ 3 ] ++ ;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(lang_pair1)\n",
    "\n",
    "lang_pair1 = (\"Python\", \"C++\")\n",
    "src_codes1, target_codes1, pids1 = lang_pair_dict[lang_pair1]\n",
    "lang1, lang2 = lang_pair1\n",
    "key = 963\n",
    "print(detok_format(src_codes1[key], file_detokenizers[lang1]))\n",
    "print(detok_format(target_codes1[key], file_detokenizers[lang2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "urban-prairie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C++', 'Java') 17056\n",
      "('C++', 'Python') 42234\n",
      "('C++', 'C#') 15599\n",
      "('C++', 'C') 858\n",
      "('Java', 'Python') 16851\n",
      "('Java', 'C#') 3787\n",
      "('Java', 'C') 786\n",
      "('Python', 'C#') 9629\n",
      "('Python', 'C') 9549\n",
      "('C#', 'C') 367\n"
     ]
    }
   ],
   "source": [
    "merged_lang_pair_dict = get_merged_lang_pair_dict(lang_pair_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "stretch-cradle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mingzhu/CodeModel/CodeGen_cwd/codenet_function_pairs/'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('C++', 'Java') 17056\n",
    "# ('C++', 'Python') 41701\n",
    "# ('C++', 'C#') 15599\n",
    "# ('Java', 'Python') 16457\n",
    "# ('Python', 'C#') 9546\n",
    "# ('Java', 'C#') 3787\n",
    "# ('Python', 'C') 8731\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-province",
   "metadata": {},
   "source": [
    "#### Split into train-val-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "anonymous-bread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ Java\n",
      "C++ Python\n",
      "C++ C#\n",
      "C++ C\n",
      "Java Python\n",
      "Java C#\n",
      "Java C\n",
      "Python C#\n",
      "Python C\n",
      "C# C\n",
      "C++ 35303\n",
      "Java 9212\n",
      "Python 9233\n",
      "C# 2852\n",
      "C 8480\n"
     ]
    }
   ],
   "source": [
    "# Any constraints?\n",
    "# Simple. Just split at problem level\n",
    "all_problem_ids = get_all_problem_ids(merged_lang_pair_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "absolute-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_problem_ids_list = list(all_problem_ids)\n",
    "train_ratio = 0.85\n",
    "val_ratio = 0.05\n",
    "test_ratio = 0.1\n",
    "num_problems = len(all_problem_ids_list)\n",
    "train_num = int(train_ratio*num_problems)\n",
    "test_num = int(test_ratio*num_problems)\n",
    "train_proids = all_problem_ids_list[:train_num]\n",
    "test_proids = all_problem_ids_list[num_problems-test_num:]\n",
    "val_proids = all_problem_ids_list[train_num:num_problems-test_num]\n",
    "codenet_hypo_split_dict = {'train':train_proids, 'test':test_proids, 'val':val_proids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "seeing-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"codenet_hypo_split_dict.json\", 'w') as outfile:\n",
    "    json.dump(codenet_hypo_split_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"codenet_hypo_split_dict.json\") as infile:\n",
    "    codenet_hypo_split_dict = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "mineral-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ Java\n",
      "C++ Java [13706, 375, 2975]\n",
      "C++ Python\n",
      "C++ Python [35250, 1393, 5591]\n",
      "C++ C#\n",
      "C++ C# [12531, 427, 2641]\n",
      "C++ C\n",
      "C++ C [661, 8, 189]\n",
      "Java Python\n",
      "Java Python [13746, 644, 2461]\n",
      "Java C#\n",
      "Java C# [2908, 132, 747]\n",
      "Java C\n",
      "Java C [522, 11, 253]\n",
      "Python C#\n",
      "Python C# [7922, 271, 1436]\n",
      "Python C\n",
      "Python C [7924, 315, 1310]\n",
      "C# C\n",
      "C# C [261, 7, 99]\n"
     ]
    }
   ],
   "source": [
    "split_lang_pair_dict = get_split_lang_pair_dict(merged_lang_pair_dict, merged_filtered_dict, \n",
    "                                                codenet_hypo_split_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-assembly",
   "metadata": {},
   "source": [
    "#### Write into parallel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "appreciated-delight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ Java 242\n",
      "C++ Python 2353\n",
      "C++ C# 228\n",
      "C++ C 0\n",
      "Java Python 903\n",
      "Java C# 22\n",
      "Java C 1\n",
      "Python C# 659\n",
      "Python C 271\n",
      "C# C 0\n"
     ]
    }
   ],
   "source": [
    "size_dict = {}\n",
    "for lang1, lang2 in merged_lang_pair_dict.keys():\n",
    "    size_dict[(lang1, lang2)] = 0\n",
    "    src_codes, target_codes, pids = merged_lang_pair_dict[(lang1, lang2)]\n",
    "    for tag in tags:\n",
    "        tag_indices = split_lang_pair_dict[(lang1, lang2)][tag]\n",
    "        for tag_i in tag_indices:\n",
    "            pid = pids[tag_i]\n",
    "            src_code = src_codes[tag_i]\n",
    "            target_code = target_codes[tag_i]\n",
    "            if len(src_code) > 1000:\n",
    "                size_dict[(lang1, lang2)] += 1\n",
    "    print(lang1, lang2, size_dict[(lang1, lang2)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "previous-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "codenet_pair_path = codenet_processed_data_path + \"codenet_function_pairs_non_plbart/\"\n",
    "write_codenet_pairdata(merged_lang_pair_dict, split_lang_pair_dict, codenet_pair_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-spanking",
   "metadata": {},
   "source": [
    "#### Check alignment again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang1, lang2 in merged_lang_pair_dict.keys():\n",
    "    src_codes, target_codes, pids = merged_lang_pair_dict[(lang1, lang2)]\n",
    "    for tag in tags:\n",
    "        tag_indices = split_lang_pair_dict[(lang1, lang2)][tag]\n",
    "        tag_i = tag_indices[-1]\n",
    "        src_code = src_codes[tag_i]\n",
    "        target_code = target_codes[tag_i]\n",
    "        print(lang1, lang2)\n",
    "        print(src_code)\n",
    "        print(target_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-drive",
   "metadata": {},
   "source": [
    "### Create non-plbart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "still-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codenet_process_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-kenya",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_plbart = True\n",
    "merged_filtered_dict = get_prepro_filtered_dict(merged_filtered_dict, is_plbart)\n",
    "programs_dict = get_codenet_programs_dict(merged_filtered_dict)\n",
    "with open(cached_path + \"codet5_codenet_lang_pair_call_dict.pkl\", 'rb') as infile:\n",
    "    call_dict = pickle.load(infile)\n",
    "# lang_pair_dict = get_lang_pair_dict(call_dict, merged_filtered_dict, programs_dict, is_plbart)\n",
    "# with open(cached_path + \"codet5_codenet_src_hypo_pair_dict_plbart.pkl\", 'wb') as outfile:\n",
    "#     pickle.dump(lang_pair_dict, outfile)\n",
    "with open(cached_path + \"codet5_codenet_src_hypo_pair_dict_plbart.pkl\", 'rb') as infile:\n",
    "    lang_pair_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "armed-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"codet5_codenet_lang_pair_call_dict.pkl\", 'rb') as infile:\n",
    "    call_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "excellent-composite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Java', 'C') 1251\n",
      "('Python', 'C') 4730\n",
      "('C#', 'C') 214\n",
      "('C++', 'C') 13451\n",
      "('C', 'Java') 5844\n",
      "('C', 'Python') 7867\n",
      "('C', 'C#') 5524\n",
      "('C', 'C++') 8765\n"
     ]
    }
   ],
   "source": [
    "for lang_pair, lists in lang_pair_dict.items():\n",
    "    print(lang_pair, len(lists[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "compliant-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C++', 'Java') 0\n",
      "('C++', 'Python') 0\n",
      "('C++', 'C#') 0\n",
      "('C++', 'C') 22216\n",
      "('Java', 'Python') 0\n",
      "('Java', 'C#') 0\n",
      "('Java', 'C') 7095\n",
      "('Python', 'C#') 0\n",
      "('Python', 'C') 12597\n",
      "('C#', 'C') 5738\n",
      "C++ Java\n",
      "C++ Java [0, 0, 0]\n",
      "C++ Python\n",
      "C++ Python [0, 0, 0]\n",
      "C++ C#\n",
      "C++ C# [0, 0, 0]\n",
      "C++ C\n",
      "C++ C [17940, 507, 3769]\n",
      "Java Python\n",
      "Java Python [0, 0, 0]\n",
      "Java C#\n",
      "Java C# [0, 0, 0]\n",
      "Java C\n",
      "Java C [5599, 117, 1379]\n",
      "Python C#\n",
      "Python C# [0, 0, 0]\n",
      "Python C\n",
      "Python C [10282, 362, 1953]\n",
      "C# C\n",
      "C# C [4598, 87, 1053]\n"
     ]
    }
   ],
   "source": [
    "merged_lang_pair_dict = get_merged_lang_pair_dict(lang_pair_dict)\n",
    "codenet_pair_path = codenet_processed_data_path + \"codet5_codenet_function_pairs/\"\n",
    "if not os.path.exists(codenet_pair_path):\n",
    "    os.mkdir(codenet_pair_path)\n",
    "split_lang_pair_dict = get_split_lang_pair_dict(merged_lang_pair_dict, merged_filtered_dict, \n",
    "                                                codenet_hypo_split_dict)\n",
    "write_codenet_pairdata(merged_lang_pair_dict, split_lang_pair_dict, codenet_pair_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "sustainable-premises",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C++', 'Java') 17056\n",
      "('C++', 'Python') 42234\n",
      "('C++', 'C#') 15599\n",
      "('C++', 'C') 858\n",
      "('Java', 'Python') 16851\n",
      "('Java', 'C#') 3787\n",
      "('Java', 'C') 786\n",
      "('Python', 'C#') 9629\n",
      "('Python', 'C') 9549\n",
      "('C#', 'C') 367\n"
     ]
    }
   ],
   "source": [
    "is_plbart = True\n",
    "\n",
    "merged_filtered_dict = get_prepro_filtered_dict(merged_filtered_dict, is_plbart)\n",
    "programs_dict = get_codenet_programs_dict(merged_filtered_dict)\n",
    "with open(cached_path + \"codenet_lang_pair_call_dict_new.pkl\", 'rb') as infile:\n",
    "    call_dict = pickle.load(infile)\n",
    "# lang_pair_dict = get_lang_pair_dict(call_dict, merged_filtered_dict, programs_dict, is_plbart)\n",
    "with open(cached_path + \"codenet_src_hypo_pair_dict_plbart.pkl\", 'rb') as infile:\n",
    "    lang_pair_dict = pickle.load(infile)\n",
    "merged_lang_pair_dict = get_merged_lang_pair_dict(lang_pair_dict)\n",
    "codenet_pair_path = codenet_processed_data_path + \"codenet_function_pairs/\"\n",
    "write_codenet_pairdata(merged_lang_pair_dict, split_lang_pair_dict, codenet_pair_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-moment",
   "metadata": {},
   "source": [
    "#### Create a smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "known-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_plbart = True\n",
    "merged_filtered_dict = get_prepro_filtered_dict(merged_filtered_dict, is_plbart)\n",
    "programs_dict = get_codenet_programs_dict(merged_filtered_dict)\n",
    "with open(cached_path + \"codenet_src_hypo_pair_dict_plbart.pkl\", 'rb') as infile:\n",
    "    lang_pair_dict = pickle.load(infile)\n",
    "# merged_lang_pair_dict = get_merged_lang_pair_dict(lang_pair_dict)\n",
    "# codenet_pair_path = codenet_processed_data_path + \"codenet_function_pairs_small/\"\n",
    "# write_codenet_pairdata(merged_lang_pair_dict, split_lang_pair_dict, codenet_pair_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lang_pair_dict = {}\n",
    "iterated_set = set()\n",
    "for lang1 in new_langs:\n",
    "    for lang2 in new_langs:\n",
    "        if lang2 == lang1:\n",
    "            continue\n",
    "        lang_pair1 = (lang1, lang2)\n",
    "        if lang_pair1 in iterated_set:\n",
    "            continue\n",
    "        lang_pair2 = (lang2, lang1)\n",
    "        iterated_set.add(lang_pair1)\n",
    "        iterated_set.add(lang_pair2)\n",
    "        \n",
    "        src_codes1, target_codes1, pids1 = [], [], []\n",
    "        src_codes2, target_codes2, pids2 = [], [], []\n",
    "        if lang_pair1 in lang_pair_dict:\n",
    "            src_codes1, target_codes1, pids1 = lang_pair_dict[lang_pair1]\n",
    "        if lang_pair2 in lang_pair_dict:\n",
    "            src_codes2, target_codes2, pids2 = lang_pair_dict[lang_pair2]\n",
    "        src_codes, target_codes, pids = src_codes1, target_codes1, pids1\n",
    "        pids = [lang1 + \"-\" + str(x) for x in pids1]\n",
    "        if len(src_codes1) > len(src_codes2):\n",
    "            src_codes, target_codes, pids = target_codes2, src_codes2, pids2\n",
    "            pids = [lang2 + \"-\" + str(x) for x in pids2]\n",
    "        merged_lang_pair_dict[lang_pair1] = [src_codes, target_codes, pids]\n",
    "        print(lang_pair1, len(pids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lang_pair_dict = get_split_lang_pair_dict(merged_lang_pair_dict, merged_filtered_dict, \n",
    "                                                codenet_hypo_split_dict)\n",
    "codenet_pair_path = codenet_processed_data_path + \"codenet_function_pairs_small/\"\n",
    "if not os.path.exists(codenet_pair_path):\n",
    "    os.mkdir(codenet_pair_path)\n",
    "write_codenet_pairdata(merged_lang_pair_dict, split_lang_pair_dict, codenet_pair_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-arctic",
   "metadata": {},
   "source": [
    "### Eval model trained on the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use what to eval? if use xlcost, it's not fair because there's domain shift.\n",
    "# Should eval on leetcode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-update",
   "metadata": {},
   "source": [
    "### Resampling for pids that doesn't have filtered hypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resamples for pids in empty_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "frank-walter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 76.87it/s]\n"
     ]
    }
   ],
   "source": [
    "lang1 = \"C#\"\n",
    "lang2 = \"C++\"\n",
    "preds = preds_lang_dict_all[(lang1, lang2)][:100]\n",
    "new_preds = get_dedup_preds(preds)\n",
    "functions, function_id_dict = prep_exec_hypo_codenet(new_preds, lang1, lang2, \n",
    "                                                            merged_filtered_dict, model_type)\n",
    "# call_list = get_hypo_call_list(functions, lang2)\n",
    "# filtered_dict = get_compiled_hypos(call_list, function_id_dict, merged_filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sustained-carry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f550c3aacc74c209afe0f7cbb87ad79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=426.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C++ error 348 0.8169014084507042\n",
      "C++ timeout 0 0.0\n",
      "C++ empty 78 0.18309859154929578\n",
      "C++ other 0 0.0\n",
      "C++ good 0 0.0\n"
     ]
    }
   ],
   "source": [
    "call_list = get_hypo_call_list(functions, lang2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict = get_compiled_hypos(call_list, function_id_dict, merged_filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (error_key, error)  in enumerate(zip(result_keys, processed_results)):\n",
    "    if error_key == \"error\":\n",
    "        if \"Compilation failed\" in error:\n",
    "            print(i, error)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-noise",
   "metadata": {},
   "source": [
    "### Codenet data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-failure",
   "metadata": {},
   "source": [
    "#### Get input stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-commissioner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_batch = 41\n",
    "code_lang_dict_list = []\n",
    "len_dict = {}\n",
    "for i in tqdm(range(num_batch)):\n",
    "    codedict_path = cached_path + 'codenet/codenet_codedict_' + str(i) + '.json'\n",
    "    if os.path.exists(codedict_path):\n",
    "        with open(cached_path + 'codenet/codenet_problems_dict_' + str(i) + '.json') as infile:\n",
    "            codenet_problems_dict_batch = json.load(infile)\n",
    "        programs_dict, programs_idx_dict, program_id_dict = get_codenet_programs(\n",
    "                                                                codenet_problems_dict_batch, new_langs)\n",
    "        len_dict[i] = {}\n",
    "        for lang in new_langs:\n",
    "            len_dict[i][lang] = len(programs_dict[lang])\n",
    "            print(lang, len(programs_dict[lang]))\n",
    "#         code_lang_dict = get_codenet_code_dict(programs_dict, programs_idx_dict, program_id_dict, \n",
    "#                                            codenet_problems_dict_batch)\n",
    "\n",
    "        with open(codedict_path) as infile:\n",
    "            code_lang_dict = json.load(infile)\n",
    "        code_lang_dict_list.append(code_lang_dict)\n",
    "#         for lang in new_langs:\n",
    "#             print(lang, len(code_lang_dict[lang]))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "acute-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_lang_dict = {x:0 for x in new_langs}\n",
    "for i, dic in len_dict.items():\n",
    "    for lang in new_langs:\n",
    "        len_lang_dict[lang] += dic[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "driving-vampire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C++': 525822, 'Java': 87482, 'Python': 132316, 'C#': 19362, 'C': 158246}\n"
     ]
    }
   ],
   "source": [
    "# raw input programs\n",
    "print(len_lang_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ignored-australian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C++': 251732, 'Java': 30709, 'Python': 28830, 'C#': 7651, 'C': 50265}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# programs with at least one function\n",
    "count_lang_dict = {x:0 for x in new_langs}\n",
    "for code_lang_dict in code_lang_dict_list:\n",
    "    for lang in new_langs:\n",
    "        dics = code_lang_dict[lang]\n",
    "        #TODO. Count dics have more than one func\n",
    "        for dic in dics:\n",
    "            if len(dic['functions']) > 0:\n",
    "                count_lang_dict[lang] += 1\n",
    "count_lang_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-queensland",
   "metadata": {},
   "source": [
    "#### Get codedicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batch = 41\n",
    "code_lang_dict_list = []\n",
    "for i in tqdm(range(26, num_batch)):\n",
    "    with open(cached_path + 'codenet_problems_dict_' + str(i) + '.json') as infile:\n",
    "        codenet_problems_dict_batch = json.load(infile)\n",
    "    programs_dict, programs_idx_dict, program_id_dict = get_codenet_programs(\n",
    "                                                            codenet_problems_dict_batch, new_langs)\n",
    "    code_lang_dict = get_codenet_code_dict(programs_dict, programs_idx_dict, program_id_dict, \n",
    "                                       codenet_problems_dict_batch)\n",
    "    with open(cached_path + 'codenet_codedict_' + str(i) + '.json', 'w') as outfile:\n",
    "        json.dump(code_lang_dict, outfile)\n",
    "    code_lang_dict_list.append(code_lang_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-pressure",
   "metadata": {},
   "source": [
    "#### Filter out programs that compiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_dict_list = []\n",
    "filtered_dict_list = []\n",
    "for code_lang_dict in tqdm(code_lang_dict_list):\n",
    "    func_id_dict, program_dict, imports_dict = get_nonempty_functions(code_lang_dict, new_langs)\n",
    "    call_dict = get_codenet_call_dict(program_dict, imports_dict, new_langs)\n",
    "    call_dict_list.append(call_dict)\n",
    "    filtered_dict = get_compiled_functions(call_dict, func_id_dict, imports_dict, program_dict, \n",
    "                                       code_lang_dict)\n",
    "    filtered_dict_list.append(filtered_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-conviction",
   "metadata": {},
   "source": [
    "#### Merge into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the filtered programs\n",
    "merge_filtered_dict(num_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-leeds",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove langs other than the 7 langs\n",
    "fns = os.listdir(codenet_data_path)\n",
    "for fn in fns:\n",
    "    lang_path = codenet_data_path + fn + '/'\n",
    "    lang_fns = os.listdir(lang_path)\n",
    "    for lang_fn in lang_fns:\n",
    "        print(lang_fn)\n",
    "        if lang_fn not in langs:\n",
    "            shutil.rmtree(lang_path + lang_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
