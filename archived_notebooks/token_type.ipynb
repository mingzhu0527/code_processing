{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_utils import *\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import jsonlines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-deposit",
   "metadata": {},
   "source": [
    "# Token Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "program_data_map = \"/home/mingzhu/CodeModel/g4g/map_data/\"\n",
    "token_type_path = \"/home/mingzhu/CodeModel/CodeGen/token_type/new_codenet_tokenized_data/\"\n",
    "path_token = token_type_path + \"token2type.pickle\"\n",
    "path_type = token_type_path + \"type2token.pickle\"\n",
    "path_bpe = token_type_path + \"token2bpe.pickle\"\n",
    "langs = [\"C++\", \"Java\", \"Python\", \"C#\", \"Javascript\", \"PHP\", \"C\"]\n",
    "for lang in langs:\n",
    "    tokenized_type_path = token_type_path + lang + \"/\"\n",
    "\n",
    "def get_programs(lang):\n",
    "    fn = program_data_map + lang + \"-program-tok.jsonl\"\n",
    "    with open(fn) as infile:\n",
    "        lines = infile.readlines()\n",
    "    programs = []\n",
    "    program_tokens = []\n",
    "    for line in lines:\n",
    "        dic = json.loads(line.strip())\n",
    "        program_raw = \" \".join(dic['tokens'])\n",
    "        line = program_raw.replace(\"@@ \", \"\")\n",
    "        program = detok_format(line, file_detokenizers[lang])\n",
    "        programs.append(program)\n",
    "        program_tokens.append(dic['tokens'])\n",
    "    return programs, program_tokens\n",
    "\n",
    "\n",
    "with open(path_token, 'rb') as infile:\n",
    "    dic_token = pickle.load(infile)\n",
    "with open(path_type, 'rb') as infile:\n",
    "    dic_type = pickle.load(infile)\n",
    "with open(path_bpe, 'rb') as infile:\n",
    "    dic_bpe = pickle.load(infile)\n",
    "\n",
    "lang = \"Java\"\n",
    "java_programs, java_program_tokens = get_programs(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_bpe_token = {lang:[] for lang in langs}\n",
    "# 判断当前是不是identifier，还要排除string的情况（string的话，上一个token应该是\"或者'）\n",
    "# token2bpe好像有Bug, \" Tree ▁ is ▁ foldable \" ['\"', 'Tree', '▁', 'is', 'fold@@', 'able']， 怎么少了一个“？\n",
    "dict_bpe_type = {lang:[] for lang in langs}\n",
    "for lang in langs:\n",
    "    for token, bpe_list in dic_bpe[lang].items():\n",
    "        for bpe in bpe_list:\n",
    "            if token in dic_token[lang]:\n",
    "                if \"\\\"\" in bpe_list:\n",
    "                    \n",
    "                if bpe not in dict_bpe_type:\n",
    "                    dict_bpe_type[bpe] = dic_token[lang][token]\n",
    "                elif dict_bpe_type[bpe] != dic_token[lang][token]:\n",
    "                    print(\"dup\", bpe, token, bpe_list, dic_token[lang][token], dict_bpe_type[bpe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dic_token = dic_token[lang]\n",
    "good_identifier_set = set()\n",
    "line_dic = {i+1:[] for i in range(4)}\n",
    "for program in java_programs:\n",
    "#     print(program)\n",
    "    lines = program.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split()\n",
    "        rule_set = set()\n",
    "        new_set = set()\n",
    "        \n",
    "#         if \"=\" in tokens or tokens[0] in set([\"int\", \"String\"]):\n",
    "#             if tokens[0] not in set(['return', 'import', 'for', 'if', 'else', \"while\", 'System', 'tree']):\n",
    "#                 rule_set.add(line)\n",
    "        if tokens[0] not in lang_dic_token:\n",
    "            continue\n",
    "        elif lang_dic_token[tokens[0]][0] == \"identifier\":\n",
    "            bad_cases_identifier = set(['tree', 'System', 'dfs'])\n",
    "            if tokens[-1] in [';', '{']:\n",
    "                if tokens[0] not in bad_cases_identifier:\n",
    "                    if tokens[0][0].isupper():\n",
    "                        if len(tokens) > 1:\n",
    "                            if tokens[1] == '.':\n",
    "                                continue\n",
    "                        if len(tokens) > 2:\n",
    "                            if tokens[1] == '[' and tokens[2] != ']':\n",
    "                                continue\n",
    "                        good_identifier_set.add(tokens[0])\n",
    "                        line_dic[1].append(line)\n",
    "                    elif '=' in tokens:\n",
    "                        good_identifier_set.add(tokens[0])\n",
    "                        line_dic[2].append(line)\n",
    "   \n",
    "        elif lang_dic_token[tokens[0]][0] == \"keyword\" :\n",
    "            bad_cases_keyword = set(['return', 'import', 'for', 'if', 'else', \"while\"])\n",
    "            good_cases_keyword = set(['boolean', 'char', 'class', 'double', \n",
    "                                      'final', 'float', 'int', 'long', 'new', \n",
    "                                      'private', 'public', 'static', 'void', 'this'])\n",
    "            if tokens[0] in good_cases_keyword:\n",
    "                if tokens[-1] in [';', '{']:\n",
    "                    if '=' in tokens:\n",
    "                        line_dic[3].append(line)\n",
    "                    else:\n",
    "                        line_dic[4].append(line)\n",
    "#         for x in rule_set:\n",
    "#             if x not in new_set:\n",
    "#                 print(\"aaa\", x)\n",
    "        \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_define_java(line, lang_dic_token):\n",
    "    tokens = line.strip().split()\n",
    "    flag = False\n",
    "    if tokens[0] not in lang_dic_token:\n",
    "        return flag\n",
    "    elif lang_dic_token[tokens[0]][0] == \"identifier\":\n",
    "        bad_cases_identifier = set(['tree', 'System', 'dfs'])\n",
    "        if tokens[0] not in bad_cases_identifier:\n",
    "            if tokens[0][0].isupper():\n",
    "                if len(tokens) > 1:\n",
    "                    if tokens[1] == '.':\n",
    "                        return flag\n",
    "                if len(tokens) > 2:\n",
    "                    if tokens[1] == '[' and tokens[2] != ']':\n",
    "                        return flag\n",
    "                flag = True\n",
    "    elif lang_dic_token[tokens[0]][0] == \"keyword\" :\n",
    "        good_cases_keyword = set(['boolean', 'char', 'class', 'double', \n",
    "                                  'final', 'float', 'int', 'long', 'new', \n",
    "                                  'private', 'public', 'static', 'void', 'this'])\n",
    "        if tokens[0] in good_cases_keyword:\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "is_define_java(\"class BinaryTree \", lang_dic_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dic_token = dic_token[lang]\n",
    "\n",
    "for program, program_tokens in zip(java_programs, java_program_tokens):\n",
    "    lines = program.split(\"\\n\")\n",
    "    program_raw_tokens = []\n",
    "    for line in lines:\n",
    "        program_raw_tokens.extend(line.split())\n",
    "        program_raw_tokens.append('\\n')\n",
    "    \n",
    "    last_line = lines[0]\n",
    "    line_n = 0\n",
    "    for token_raw in program_raw_tokens:\n",
    "        if token_raw == \"\\n\":\n",
    "            line_n += 1\n",
    "            if line_n < len(lines) - 1:\n",
    "                last_line = lines[line_n]\n",
    "            continue\n",
    "        if token_raw in dic_bpe[lang]:\n",
    "            bpe_tokens = dic_bpe[lang][token_raw]\n",
    "            for i, token in enumerate(bpe_tokens):\n",
    "                if (token in lang_dic_token and \n",
    "                    lang_dic_token[token] == \"identifier\") or ((token.endswith(\"@@\") \n",
    "                                                                    or bpe_tokens[i-1].endswith(\"@@\")) \n",
    "                                                                    and \"\\\"\" not in last_line\n",
    "                                                                    and \"\\'\" not in last_line):\n",
    "\n",
    "#                     print(last_line, token_raw, token)\n",
    "                    pass\n",
    "        elif not (\"\\\"\" in token_raw or \"\\'\" in token_raw):\n",
    "            print(\"Exception\", last_line, token_raw)\n",
    "\n",
    "        # 这里判断identifier可能有些问题，因为string也会有@@\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import javalang\n",
    "tokens = javalang.tokenizer.tokenize('gap == ( gap * 10 );')\n",
    "parser = javalang.parser.Parser(tokens)\n",
    "parser.parse_expression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 2\n",
    "for line in line_dic[key]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "import javalang\n",
    "source = \"\"\"import java . io . * ;\n",
    "import java . util . * ;\n",
    "class CountTriangles {\n",
    "  static int findNumberOfTriangles ( int arr [ ] ) {\n",
    "    int n = arr . length ;\n",
    "    Arrays . sort ( arr ) ;\n",
    "    int count = 0 ;\n",
    "    for ( int i = 0 ; i < n - 2 ; ++ i ) {\n",
    "      int k = i + 2 ;\n",
    "      for ( int j = i + 1 ; j < n ; ++ j ) {\n",
    "        while ( k < n && arr [ i ] + arr [ j ] > arr [ k ] ) ++ k ;\n",
    "        if ( k > j ) count += k - j - 1 ;\n",
    "      }\n",
    "    }\n",
    "    return count ;\n",
    "  }\n",
    "  public static void main ( String [ ] args ) {\n",
    "      ArrayList<String> cars = new ArrayList<String>();\n",
    "      int arr[] = {\n",
    "      10 , 21 , 22 , 100 , 101 , 200 , 300 };\n",
    "      System . out . println ( \"Total number of triangles is \" + findNumberOfTriangles ( arr ) ) ;\n",
    "    }\n",
    "  }\"\"\"\n",
    "\n",
    "source_tokens = []\n",
    "types = set()\n",
    "for token in javalang.tokenizer.tokenize(source):\n",
    "    source_tokens.append((type(token).__name__, token.value))\n",
    "    types.add(type(token).__name__)\n",
    "print(source_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
