{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "healthy-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instant-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./huggingface_models/')\n",
    "sys.path.append('./utils/')\n",
    "from sample_utils import *\n",
    "from inference_utils import *\n",
    "from codenet_process_utils import *\n",
    "from self_training_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conditional-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-imaging",
   "metadata": {},
   "source": [
    "### Hypo Processing\n",
    "1. Preprocess filtered hypos: get_lang_pair_dict\\\n",
    "    1.1 No-tok preprocessing\n",
    "2. Merge lang1-lang2 and lang2-lang1\n",
    "3. Split in to train/val/test: get_split_lang_pair_dict\n",
    "4. Write into parallel files: write_codenet_pairdata\n",
    "\n",
    "We get lang_pair_dict in this step.\\\n",
    "```\n",
    "lang_pair_list = [src_codes, target_codes, pids]\n",
    "lang_pair_dict[(lang1, lang2)] = lang_pair_list\n",
    "```\n",
    "\n",
    "### Cached Files\n",
    "\n",
    "- Hypo call_dict\\\n",
    "    plbart_codenet_lang_pair_call_dict.pkl\n",
    "- Generated Parallel data\\\n",
    "    codet5_codenet_src_hypo_pair_dict_plbart.pkl\n",
    "- Hypo split_dict\\\n",
    "    codenet_hypo_split_dict.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-apache",
   "metadata": {},
   "source": [
    "### Load No-tok Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proprietary-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_plbart = True\n",
    "merged_filtered_dict = get_prepro_filtered_dict(None, is_plbart)\n",
    "programs_dict = get_codenet_programs_dict(merged_filtered_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-saturday",
   "metadata": {},
   "source": [
    "#### Get Import Str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "constitutional-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_str_dict = {}\n",
    "for lang in new_langs:\n",
    "    all_imports, import_str = get_common_imports(lang, merged_filtered_dict)\n",
    "    import_str_dict[lang] = import_str\n",
    "import_str_dict[\"Java\"] = java_imports_str\n",
    "import_str_dict[\"C#\"] = csharp_imports_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-alaska",
   "metadata": {},
   "source": [
    "### Filtered Hypo Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-broadway",
   "metadata": {},
   "source": [
    "#### Get all the filtered hypos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raising-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"plbart_full_codenet_lang_pair_call_dict.pkl\", 'rb') as infile:\n",
    "    call_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developed-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lang_dict = {}\n",
    "for lang1, lang2 in call_dict.keys():\n",
    "    new_preds, functions, function_id_dict, call_list = call_dict[(lang1, lang2)] \n",
    "    filtered_dict = get_compiled_hypos(call_list, function_id_dict, merged_filtered_dict)\n",
    "    filtered_lang_dict[(lang1, lang2)] = filtered_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-belgium",
   "metadata": {},
   "source": [
    "#### Separate pids that have filtered hypos and pids that don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advisory-moore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ Java 69896\n",
      "C++ C# 69052\n",
      "C++ Python 188378\n",
      "Java C++ 7411\n",
      "Java C# 10871\n",
      "Java Python 27955\n",
      "C# C++ 2149\n",
      "C# Java 2043\n",
      "C# Python 16534\n"
     ]
    }
   ],
   "source": [
    "empty_dict = {}\n",
    "non_empty_dict = {}\n",
    "for lang1, lang2 in filtered_lang_dict.keys():\n",
    "    non_empty_dict[(lang1, lang2)] = []\n",
    "    empty_dict[(lang1, lang2)] = []\n",
    "    filtered_dict = filtered_lang_dict[(lang1, lang2)]\n",
    "    for pid, inds in filtered_dict.items():\n",
    "        if len(inds) > 0:\n",
    "            non_empty_dict[(lang1, lang2)].append(pid)\n",
    "        else:\n",
    "            empty_dict[(lang1, lang2)].append(pid)\n",
    "    print(lang1, lang2, len(non_empty_dict[(lang1, lang2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-passage",
   "metadata": {},
   "source": [
    "### Check filtered hypo quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brown-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare input output and check quality\n",
    "# Quality is very good!\n",
    "# numerics translation is not accurate.\n",
    "lang1 = 'C++'\n",
    "lang2 = 'Python'\n",
    "new_preds, functions, function_id_dict, call_list = call_dict[(lang1, lang2)] \n",
    "filtered_dict = filtered_lang_dict[(lang1, lang2)]\n",
    "src_codes = programs_dict[lang1]\n",
    "src_codes_formatted = [x['function'] for x in merged_filtered_dict[lang1]]\n",
    "non_empty_list = non_empty_dict[(lang1, lang2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 112\n",
    "for lang1 in new_langs:\n",
    "    for lang2 in new_langs:\n",
    "        if lang2 == lang1:\n",
    "            continue\n",
    "        if (lang1, lang2) not in call_dict:\n",
    "            continue\n",
    "        new_preds, functions, function_id_dict, call_list = call_dict[(lang1, lang2)] \n",
    "        filtered_dict = filtered_lang_dict[(lang1, lang2)]\n",
    "        src_codes = programs_dict[lang1]\n",
    "        src_codes_formatted = [x['function'] for x in merged_filtered_dict[lang1]]\n",
    "        non_empty_list = non_empty_dict[(lang1, lang2)]\n",
    "        key = non_empty_list[-1]\n",
    "        if len(filtered_dict[key]) > 0:\n",
    "            print(lang1, lang2)\n",
    "            print(detok_format(functions[filtered_dict[key][0]], file_detokenizers[lang2]))\n",
    "            print(src_codes_formatted[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-adolescent",
   "metadata": {},
   "source": [
    "### Generate Parallel Training Data from Filtered Hypo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-forty",
   "metadata": {},
   "source": [
    "#### Preprocess Filtered Hypos\n",
    "1. functions, notok_prepro(codestring, lang, is_plbart)\n",
    "2. remove empty lines (caused by tokenization)\n",
    "3. save into paired files; create map_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-bibliography",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ Java\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331f98536f3940969aa4bcc04ee87b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=69896.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C++ C#\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba23410fb0d4b50851289dc40ef70bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=69052.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C++ Python\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55c0f8d71184fd68de2bb284d59460a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=188378.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Java C++\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1df1f7db2c4896b06e8351fc2a2525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7411.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Java C#\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6827c2b5b83f45c5901f3cdbe2bcb277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10871.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Java Python\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27115634d60049fdb131245dca8c8b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27955.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C# C++\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56dfc94033a480f8f70a55a0ea7fc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2149.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C# Java\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb3c93527ce4b7587e61a226b99cb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C# Python\n"
     ]
    }
   ],
   "source": [
    "lang_pair_dict = get_lang_pair_dict(call_dict, merged_filtered_dict, programs_dict, is_plbart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "middle-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"plbart_full_codenet_src_hypo_pair_dict_plbart.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(lang_pair_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-explorer",
   "metadata": {},
   "source": [
    "#### Check alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "loved-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"plbart_full_codenet_src_hypo_pair_dict_plbart.pkl\", 'rb') as infile:\n",
    "    lang_pair_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "waiting-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('C++', 'Java'), ('C++', 'C#'), ('C++', 'Python'), ('Java', 'C++'), ('Java', 'C#'), ('Java', 'Python'), ('C#', 'C++'), ('C#', 'Java'), ('C#', 'Python')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_pair_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_pair1 = (\"Java\", \"Python\")\n",
    "lang_pair2 = (\"Python\", \"Java\")\n",
    "src_codes1, target_codes1, pids1 = lang_pair_dict[lang_pair1]\n",
    "src_codes2, target_codes2, pids2 = lang_pair_dict[lang_pair2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "clear-eugene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Java', 'Python')\n",
      "private static ArrayList < Integer > sum_dig ( ArrayList < Integer > a ) {\n",
      "  ArrayList < Integer > b = new ArrayList < Integer > ( ) ;\n",
      "  for ( int i = 0 ;\n",
      "  i < a . size ( ) / 2 ; i ++ ) {\n",
      "    int sum = 0 ;\n",
      "    if ( i == 0 ) sum = a . get ( 0 ) + a . get ( 1 ) ;\n",
      "    sum = a . get ( i * 2 ) + a . get ( i * 2 + 1 ) ;\n",
      "    String sums ;\n",
      "    sums = String . valueOf ( sum ) ;\n",
      "    b . add ( sums . length ( ) ) ;\n",
      "  }\n",
      "  return b ;\n",
      "}\n",
      "private static void print_dig ( ArrayList < Integer > a ) {\n",
      "  for ( int i = 0 ;\n",
      "  i < a . size ( ) ; i ++ ) {\n",
      "    System . out . println ( a . get ( i ) ) ;\n",
      "  }\n",
      "}\n",
      "def sum_dig ( a ) :\n",
      "    b = [ ]\n",
      "    for i in range ( 0 , int ( len ( a ) / 2 ) , 1 ) :\n",
      "        sum = 0\n",
      "        if ( i == 0 ) :\n",
      "            sum = a [ 0 ] + a [ 1 ]\n",
      "        sum = a [ i * 2 ] + a [ i * 2 + 1 ]\n",
      "        sums = str ( sum )\n",
      "        b.append ( len ( sums ) )\n",
      "    return b\n",
      "def print_dig ( a ) :\n",
      "    for i in range ( 0 , len ( a ) , 1 ) :\n",
      "        print ( a [ i ] , end = \"\" )\n"
     ]
    }
   ],
   "source": [
    "print(lang_pair1)\n",
    "lang_pair1 = (\"Java\", \"Python\")\n",
    "src_codes1, target_codes1, pids1 = lang_pair_dict[lang_pair1]\n",
    "lang1, lang2 = lang_pair1\n",
    "key = 123\n",
    "print(detok_format(src_codes1[key], file_detokenizers[lang1]))\n",
    "print(detok_format(target_codes1[key], file_detokenizers[lang2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-fellowship",
   "metadata": {},
   "source": [
    "#### Merge lang1-lang2 and lang2-lang1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "urban-prairie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C++', 'Java') 77307\n",
      "('C++', 'Python') 188378\n",
      "('C++', 'C#') 71201\n",
      "('C++', 'C') 0\n",
      "('Java', 'Python') 27955\n",
      "('Java', 'C#') 12914\n",
      "('Java', 'C') 0\n",
      "('Python', 'C#') 16534\n",
      "('Python', 'C') 0\n",
      "('C#', 'C') 0\n"
     ]
    }
   ],
   "source": [
    "merged_lang_pair_dict = get_merged_lang_pair_dict(lang_pair_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-province",
   "metadata": {},
   "source": [
    "#### Load Hypo split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "verified-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any constraints?\n",
    "# Simple. Just split at problem level\n",
    "with open(cached_path + \"codenet_hypo_split_dict.json\") as infile:\n",
    "    codenet_hypo_split_dict = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "mineral-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ Java\n",
      "C++ Java [65579, 5413, 6315]\n",
      "C++ Python\n",
      "C++ Python [161332, 11832, 15214]\n",
      "C++ C#\n",
      "C++ C# [60566, 5128, 5507]\n",
      "C++ C\n",
      "C++ C [0, 0, 0]\n",
      "Java Python\n",
      "Java Python [23589, 1774, 2592]\n",
      "Java C#\n",
      "Java C# [10748, 899, 1267]\n",
      "Java C\n",
      "Java C [0, 0, 0]\n",
      "Python C#\n",
      "Python C# [14292, 871, 1371]\n",
      "Python C\n",
      "Python C [0, 0, 0]\n",
      "C# C\n",
      "C# C [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "split_lang_pair_dict = get_split_lang_pair_dict(merged_lang_pair_dict, merged_filtered_dict, \n",
    "                                                codenet_hypo_split_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-assembly",
   "metadata": {},
   "source": [
    "#### Write into parallel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "previous-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "codenet_pair_path = codenet_processed_data_path + \"codenet_function_pairs_non_plbart/\"\n",
    "write_codenet_pairdata(merged_lang_pair_dict, split_lang_pair_dict, codenet_pair_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-spanking",
   "metadata": {},
   "source": [
    "#### Check Parallel File Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check last line\n",
    "for lang1, lang2 in merged_lang_pair_dict.keys():\n",
    "    src_codes, target_codes, pids = merged_lang_pair_dict[(lang1, lang2)]\n",
    "    for tag in tags:\n",
    "        tag_indices = split_lang_pair_dict[(lang1, lang2)][tag]\n",
    "        tag_i = tag_indices[-1]\n",
    "        src_code = src_codes[tag_i]\n",
    "        target_code = target_codes[tag_i]\n",
    "        print(lang1, lang2)\n",
    "        print(src_code)\n",
    "        print(target_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-drive",
   "metadata": {},
   "source": [
    "### Create Parallel Dataset (summary of the above steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-kenya",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_plbart = False\n",
    "merged_filtered_dict = get_prepro_filtered_dict(None, is_plbart)\n",
    "programs_dict = get_codenet_programs_dict(merged_filtered_dict)\n",
    "with open(cached_path + \"codet5_codenet_lang_pair_call_dict.pkl\", 'rb') as infile:\n",
    "    call_dict = pickle.load(infile)\n",
    "codenet_pair_path = codenet_processed_data_path + \"codet5_codenet_function_pairs/\"\n",
    "if not os.path.exists(codenet_pair_path):\n",
    "    os.mkdir(codenet_pair_path)\n",
    "lang_pair_dict_path = cached_path + \"codet5_codenet_src_hypo_pair_dict_plbart.pkl\"\n",
    "if os.path.exists(lang_pair_dict_path):\n",
    "    with open(lang_pair_dict_path, 'rb') as infile:\n",
    "        lang_pair_dict = pickle.load(infile)\n",
    "else:\n",
    "    lang_pair_dict = get_lang_pair_dict(call_dict, merged_filtered_dict, programs_dict, is_plbart)\n",
    "    with open(lang_pair_dict_path, 'wb') as outfile:\n",
    "        pickle.dump(lang_pair_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_pair, lists in lang_pair_dict.items():\n",
    "    print(lang_pair, len(lists[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lang_pair_dict = get_merged_lang_pair_dict(lang_pair_dict)\n",
    "split_lang_pair_dict = get_split_lang_pair_dict(merged_lang_pair_dict, merged_filtered_dict, \n",
    "                                                codenet_hypo_split_dict)\n",
    "write_codenet_pairdata(merged_lang_pair_dict, split_lang_pair_dict, codenet_pair_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
