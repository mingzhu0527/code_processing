{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "healthy-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instant-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./huggingface_models/')\n",
    "sys.path.append('./utils/')\n",
    "from sample_utils import *\n",
    "from inference_utils import *\n",
    "from codenet_process_utils import *\n",
    "from self_training_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conditional-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-region",
   "metadata": {},
   "source": [
    "### This notebook contains:\n",
    "- Show Graph\n",
    "- Split into batches for sampling\n",
    "- Create a smaller dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-empty",
   "metadata": {},
   "source": [
    "### Load Filtered Data (Filtered accepted codenet data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ambient-stable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ 195942\n",
      "Java 32053\n",
      "Python 261486\n",
      "C# 17716\n",
      "C 26547\n"
     ]
    }
   ],
   "source": [
    "with open(cached_path + 'codenet_merged_filtered_dict.json') as infile:\n",
    "    merged_filtered_dict = json.load(infile)\n",
    "for lang in new_langs:\n",
    "    print(lang, len(merged_filtered_dict[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-running",
   "metadata": {},
   "source": [
    "#### Get programs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "programs_dict = get_codenet_programs_dict(merged_filtered_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-friend",
   "metadata": {},
   "source": [
    "#### Get import_str_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_str_dict = {}\n",
    "for lang in new_langs:\n",
    "    all_imports, import_str = get_common_imports(lang, merged_filtered_dict)\n",
    "    import_str_dict[lang] = import_str\n",
    "import_str_dict[\"Java\"] = java_imports_str\n",
    "import_str_dict[\"C#\"] = csharp_imports_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-tuning",
   "metadata": {},
   "source": [
    "### Show Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"Java\"\n",
    "key = 9981 #9981\n",
    "code_dic = merged_filtered_dict[lang][key]['code_dic']\n",
    "program = code_dic['program_formatted']\n",
    "paras = code_dic['parameter_lists']\n",
    "return_types = code_dic['return_types']\n",
    "function_names = code_dic['function_names']\n",
    "functions = code_dic['functions']\n",
    "function = \"\\n\".join(functions)\n",
    "pieces = code_dic['program_pieces']\n",
    "piece = \"\".join(pieces)\n",
    "target_call = code_dic['target_call']\n",
    "# print(program)\n",
    "print(function)\n",
    "print(piece)\n",
    "print(function_names)\n",
    "print(return_types)\n",
    "print(paras)\n",
    "print(target_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"Java\"\n",
    "root1, graph1, graph_pruned1, graph_sibs1, graph_pruned_sibs1 = pipeline(code1, ast_parsers[lang1], lang1)\n",
    "# root1, graph1, graph_pruned1, graph_sibs1, graph_pruned_sibs1 = refine_graphs(root1, graph_pruned_sibs1)\n",
    "show_graph(root1, graph1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-cedar",
   "metadata": {},
   "source": [
    "### Split into batches for self-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "average-appreciation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C++ [0, 12375, 24750, 37126]\n",
      "Java [0, 3503, 7006, 10511]\n",
      "Python [0, 8970, 17940, 26911]\n",
      "C# [0, 1033, 2066, 3101]\n",
      "C [0, 4141, 8282, 12424]\n"
     ]
    }
   ],
   "source": [
    "# Java-Python\n",
    "# 3 batches \n",
    "num_batchs = 3\n",
    "batch_split_dict = {}\n",
    "for lang in new_langs:\n",
    "    length = len(merged_filtered_dict[lang])\n",
    "    batch_size = length//num_batchs\n",
    "    batch_list = [i*batch_size for i in range(num_batchs+1)]\n",
    "    batch_list[-1] = length\n",
    "    batch_split_dict[lang] = batch_list\n",
    "    print(lang, batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caring-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_programs_dict = {}\n",
    "for lang in new_langs:\n",
    "    batch_list = batch_split_dict[lang]\n",
    "    batch_functions = []\n",
    "    for bid in range(num_batchs):\n",
    "        batch_dict = merged_filtered_dict[lang][batch_list[bid]:batch_list[bid+1]]\n",
    "        functions = []\n",
    "        for dic in batch_dict:\n",
    "            functions.append(dic['function_notok'])\n",
    "        batch_functions.append(functions)\n",
    "    batch_programs_dict[lang] = batch_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "coordinate-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1 = \"Java\"\n",
    "lang2 = \"Python\"\n",
    "batch_id = 0\n",
    "src_codes = batch_programs_dict[lang1][batch_id]\n",
    "tgt_codes = []\n",
    "# infer with src and tgt\n",
    "eval_examples, eval_features, eval_dataloader, model, tokenizer, args, decoder_sid = inference_prepro(\n",
    " lang1, lang2, model_type, device, src_codes, tgt_codes, None, tag, exp_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-drive",
   "metadata": {},
   "source": [
    "### Create Parallel Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-yorkshire",
   "metadata": {},
   "source": [
    "#### Split the hypos into train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informative-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"plbart_full_codenet_src_hypo_pair_dict_plbart.pkl\", 'rb') as infile:\n",
    "    lang_pair_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lang_pair_dict = get_merged_lang_pair_dict(lang_pair_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "serial-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any constraints?\n",
    "# Simple. Just split at problem level\n",
    "all_problem_ids = get_all_problem_ids(merged_lang_pair_dict, merged_filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comparative-creature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3086"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_problem_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "divine-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"codenet_hypo_split_dict.json\", 'r') as infile:\n",
    "    codenet_hypo_split_dict = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "respected-tracker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "train_set = set(codenet_hypo_split_dict['train'])\n",
    "test_set = set(codenet_hypo_split_dict['test'])\n",
    "val_set = set(codenet_hypo_split_dict['val'])\n",
    "print(train_set & test_set)\n",
    "print(train_set & val_set)\n",
    "print(test_set & val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "awful-proof",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617 1617\n"
     ]
    }
   ],
   "source": [
    "old_all_problem_ids_list = codenet_hypo_split_dict['train'] + \\\n",
    "    codenet_hypo_split_dict['test'] + codenet_hypo_split_dict['val']\n",
    "old_all_problem_ids = set(old_all_problem_ids_list)\n",
    "print(len(old_all_problem_ids_list), len(old_all_problem_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sorted-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_problem_ids_rem = all_problem_ids - old_all_problem_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "brief-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_problem_ids_list = list(all_problem_ids_rem)\n",
    "train_ratio = 0.85\n",
    "val_ratio = 0.05\n",
    "test_ratio = 0.1\n",
    "num_problems = len(all_problem_ids_list)\n",
    "train_num = int(train_ratio*num_problems)\n",
    "test_num = int(test_ratio*num_problems)\n",
    "train_proids = all_problem_ids_list[:train_num]\n",
    "test_proids = all_problem_ids_list[num_problems-test_num:]\n",
    "val_proids = all_problem_ids_list[train_num:num_problems-test_num]\n",
    "train_proids += codenet_hypo_split_dict['train']\n",
    "test_proids += codenet_hypo_split_dict['test']\n",
    "val_proids += codenet_hypo_split_dict['val']\n",
    "codenet_hypo_split_dict = {'train':train_proids, 'test':test_proids, 'val':val_proids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "nervous-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cached_path + \"codenet_hypo_split_dict.json\", 'w') as outfile:\n",
    "    json.dump(codenet_hypo_split_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-seminar",
   "metadata": {},
   "source": [
    "#### Create a smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_plbart = True\n",
    "merged_filtered_dict = get_prepro_filtered_dict(None, is_plbart)\n",
    "programs_dict = get_codenet_programs_dict(merged_filtered_dict)\n",
    "with open(cached_path + \"codenet_src_hypo_pair_dict_plbart.pkl\", 'rb') as infile:\n",
    "    lang_pair_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lang_pair_dict = {}\n",
    "iterated_set = set()\n",
    "for lang1 in new_langs:\n",
    "    for lang2 in new_langs:\n",
    "        if lang2 == lang1:\n",
    "            continue\n",
    "        lang_pair1 = (lang1, lang2)\n",
    "        if lang_pair1 in iterated_set:\n",
    "            continue\n",
    "        lang_pair2 = (lang2, lang1)\n",
    "        iterated_set.add(lang_pair1)\n",
    "        iterated_set.add(lang_pair2)\n",
    "        \n",
    "        src_codes1, target_codes1, pids1 = [], [], []\n",
    "        src_codes2, target_codes2, pids2 = [], [], []\n",
    "        if lang_pair1 in lang_pair_dict:\n",
    "            src_codes1, target_codes1, pids1 = lang_pair_dict[lang_pair1]\n",
    "        if lang_pair2 in lang_pair_dict:\n",
    "            src_codes2, target_codes2, pids2 = lang_pair_dict[lang_pair2]\n",
    "        src_codes, target_codes, pids = src_codes1, target_codes1, pids1\n",
    "        pids = [lang1 + \"-\" + str(x) for x in pids1]\n",
    "        if len(src_codes1) > len(src_codes2):\n",
    "            # note that target and src need to be exchanged, becasue lang_pair1 and lang_pair2 are different\n",
    "            src_codes, target_codes, pids = target_codes2, src_codes2, pids2\n",
    "            pids = [lang2 + \"-\" + str(x) for x in pids2]\n",
    "        merged_lang_pair_dict[lang_pair1] = [src_codes, target_codes, pids]\n",
    "        print(lang_pair1, len(pids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lang_pair_dict = get_split_lang_pair_dict(merged_lang_pair_dict, merged_filtered_dict, \n",
    "                                                codenet_hypo_split_dict)\n",
    "codenet_pair_path = codenet_processed_data_path + \"codenet_function_pairs_small/\"\n",
    "if not os.path.exists(codenet_pair_path):\n",
    "    os.mkdir(codenet_pair_path)\n",
    "write_codenet_pairdata(merged_lang_pair_dict, split_lang_pair_dict, codenet_pair_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
